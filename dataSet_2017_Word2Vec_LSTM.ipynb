{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijitgayen/SNA_As_Msc_projects/blob/main/dataSet_2017_Word2Vec_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJcGdmRg_eN6"
      },
      "source": [
        "# Sentence Classification using LSTM and Pretrained Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F61kUpXW_eN8"
      },
      "source": [
        "We will train and test sentence classification using LSTM, and Pretrained Word2Vec.  \n",
        "You can find visualization of our code below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Add libary"
      ],
      "metadata": {
        "id": "AbaDII3m_uMZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "eSk_2_1n_eOV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3Wzwv4cM_eOX"
      },
      "outputs": [],
      "source": [
        "# Load Pretrained Word2Vec\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some functions"
      ],
      "metadata": {
        "id": "aD0_wW5a_oFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "UHI0i3i0_eOY"
      },
      "outputs": [],
      "source": [
        "def get_max_length(df):\n",
        "    \"\"\"\n",
        "    get max token counts from train data, \n",
        "    so we use this number as fixed length input to RNN cell\n",
        "    \"\"\"\n",
        "    max_length = 0\n",
        "    for row in df['tweet']:\n",
        "        if len(row.split(\" \")) > max_length:\n",
        "            max_length = len(row.split(\" \"))\n",
        "    return max_length\n",
        "\n",
        "def get_word2vec_enc(reviews):\n",
        "    \"\"\"\n",
        "    get word2vec value for each word in sentence.\n",
        "    concatenate word in numpy array, so we can use it as RNN input\n",
        "    \"\"\"\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        tokens = review.split(\" \")\n",
        "        word2vec_embedding = embed(tokens)\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "        \n",
        "def get_padded_encoded_reviews(encoded_reviews):\n",
        "    \"\"\"\n",
        "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
        "    \"\"\"\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return padded_reviews_encoding\n",
        "\n",
        "def sentiment_encode(sentiment):\n",
        "    \"\"\"\n",
        "    return one hot encoding for Y value\n",
        "    \"\"\"\n",
        "    if sentiment == 'positive':\n",
        "        return [1,0]\n",
        "    if sentiment == 'neutral':\n",
        "        return [0.5,0.5]\n",
        "    else:\n",
        "        return [0,1]\n",
        "    \n",
        "def preprocess(df):\n",
        "    \"\"\"\n",
        "    encode text value to numeric value\n",
        "    \"\"\"\n",
        "    # encode words into word2vec\n",
        "    reviews = df['tweet'].tolist()\n",
        "    \n",
        "    encoded_reviews = get_word2vec_enc(reviews)\n",
        "    padded_encoded_reviews = get_padded_encoded_reviews(encoded_reviews)\n",
        "    # encoded sentiment\n",
        "    sentiments = df['label'].tolist()\n",
        "    encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]\n",
        "    X = np.array(padded_encoded_reviews)\n",
        "    Y = np.array(encoded_sentiment)\n",
        "    return X, Y "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8-YLayo_eOb"
      },
      "source": [
        "# Preprocess (encode text to number)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HOhHRh-D92vv",
        "outputId": "65743fbe-b5ac-4694-a50e-6afefdca3811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsDatas=pd.read_csv('/content/drive/MyDrive/2017_data_set/dataset/train/twitter-2016dev-A.txt', sep=\"\\t\", header=None, names=[\"id\",\"label\", \"tweet\"]); #skiping these two rows as they have some bad data\n",
        "testData=pd.read_csv('/content/drive/MyDrive/2017_data_set/dataset/train/twitter-2016devtest-A.txt',sep=\"\\t\", header=None, names=[\"id\",\"label\", \"tweet\"]); #test_data\n",
        "print(tweetsDatas.head())\n",
        "print(tweetsDatas.label.value_counts())\n",
        "print(testData.head())\n",
        "print(testData.label.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFgFx5fUByuu",
        "outputId": "5b11a69e-edcc-4591-e49c-759545a8193b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   id     label  \\\n",
            "0  638060586258038784   neutral   \n",
            "1  638061181823922176  positive   \n",
            "2  638083821364244480   neutral   \n",
            "3  638091450132078593  positive   \n",
            "4  638125563790557184  positive   \n",
            "\n",
            "                                               tweet  \n",
            "0  05 Beat it - Michael Jackson - Thriller (25th ...  \n",
            "1  Jay Z joins Instagram with nostalgic tribute t...  \n",
            "2  Michael Jackson: Bad 25th Anniversary Edition ...  \n",
            "3  I liked a @YouTube video http://t.co/AaR3pjp2P...  \n",
            "4  18th anniv of Princess Diana's death. I still ...  \n",
            "positive    829\n",
            "neutral     746\n",
            "negative    391\n",
            "Name: label, dtype: int64\n",
            "                   id     label  \\\n",
            "0  637641175948763136   neutral   \n",
            "1  637651487762554881   neutral   \n",
            "2  637666734300905472  negative   \n",
            "3  637668142110654468   neutral   \n",
            "4  637708370129125377  positive   \n",
            "\n",
            "                                               tweet  \n",
            "0  @SeeMonterey LOST - Sony cell phone with holid...  \n",
            "1  @PersonaSoda well yeah, that's third parties. ...  \n",
            "2  Sony rewards app is like a lot of 19 y.o femal...  \n",
            "3  @fakethom Have android tab and don't use phone...  \n",
            "4  Finally I get my ps4 back I sent it to Sony ca...  \n",
            "positive    994\n",
            "neutral     681\n",
            "negative    325\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "5a6PI6pD_eOc",
        "outputId": "740d8b1b-ac0d-4067-86a9-bbe814862b83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "610\n"
          ]
        }
      ],
      "source": [
        "df=tweetsDatas\n",
        "# max_length is used for max sequence of input\n",
        "max_length = get_max_length(df)\n",
        "print(max_length)\n",
        "\n",
        "train_X, train_Y = preprocess(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X)"
      ],
      "metadata": {
        "id": "vALH4EdmIhdU",
        "outputId": "0c11a15f-458b-48be-e995-8e7514b31f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [ 0.04849368  0.02253552 -0.03001837 ...  0.01148417 -0.00711913\n",
            "    0.07169744]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.08257177 -0.00750593  0.01259988 ... -0.01107846  0.09370738\n",
            "   -0.05649239]\n",
            "  [ 0.04446086 -0.02164922 -0.08020048 ... -0.01608738  0.08119389\n",
            "    0.02990472]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.06397844  0.06924438 -0.01906524 ...  0.06709331  0.05260783\n",
            "    0.01745548]\n",
            "  [-0.06490663 -0.04434662 -0.04786254 ... -0.00385669  0.01662072\n",
            "    0.01315502]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.01905521 -0.0505188   0.02565561 ... -0.00141199  0.05673076\n",
            "   -0.03743587]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.08018466  0.05793865 -0.04210952 ...  0.03982512  0.07958872\n",
            "   -0.06999146]\n",
            "  [-0.00483655 -0.06458678 -0.02172526 ...  0.02054301  0.0117408\n",
            "    0.01072438]\n",
            "  [-0.00426278 -0.02541808 -0.01327157 ...  0.03809534 -0.02657138\n",
            "    0.03662907]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0REz_zM_eOd"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "VhNRBzA1_eOf"
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efpd3TF__eOh"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRO932fcAVdF",
        "outputId": "d774ca92-8830-4731-a37c-d026cbea74f1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [0.5, 0.5],\n",
              "       ...,\n",
              "       [0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [1. , 0. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He_9VTLk_eOi",
        "outputId": "c82f9b5b-8414-42da-bf1a-d017c5591bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/50\n",
            "62/62 [==============================] - 17s 246ms/step - loss: 0.6532 - accuracy: 0.7869\n",
            "Epoch 2/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.6103 - accuracy: 0.7635\n",
            "Epoch 3/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.6041 - accuracy: 0.7482\n",
            "Epoch 4/50\n",
            "62/62 [==============================] - 15s 245ms/step - loss: 0.5822 - accuracy: 0.7594\n",
            "Epoch 5/50\n",
            "62/62 [==============================] - 15s 245ms/step - loss: 0.5731 - accuracy: 0.7538\n",
            "Epoch 6/50\n",
            "62/62 [==============================] - 15s 246ms/step - loss: 0.5681 - accuracy: 0.7569\n",
            "Epoch 7/50\n",
            "62/62 [==============================] - 16s 255ms/step - loss: 0.5698 - accuracy: 0.7558\n",
            "Epoch 8/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.5632 - accuracy: 0.7472\n",
            "Epoch 9/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.5540 - accuracy: 0.7584\n",
            "Epoch 10/50\n",
            "62/62 [==============================] - 15s 246ms/step - loss: 0.5564 - accuracy: 0.7655\n",
            "Epoch 11/50\n",
            "62/62 [==============================] - 15s 248ms/step - loss: 0.5490 - accuracy: 0.7655\n",
            "Epoch 12/50\n",
            "62/62 [==============================] - 15s 248ms/step - loss: 0.5518 - accuracy: 0.7574\n",
            "Epoch 13/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.5426 - accuracy: 0.7599\n",
            "Epoch 14/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.5403 - accuracy: 0.7686\n",
            "Epoch 15/50\n",
            "62/62 [==============================] - 15s 245ms/step - loss: 0.5360 - accuracy: 0.7655\n",
            "Epoch 16/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.5398 - accuracy: 0.7620\n",
            "Epoch 17/50\n",
            "62/62 [==============================] - 15s 248ms/step - loss: 0.5292 - accuracy: 0.7681\n",
            "Epoch 18/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.5267 - accuracy: 0.7655\n",
            "Epoch 19/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.5222 - accuracy: 0.7670\n",
            "Epoch 20/50\n",
            "62/62 [==============================] - 15s 248ms/step - loss: 0.5222 - accuracy: 0.7675\n",
            "Epoch 21/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.5141 - accuracy: 0.7742\n",
            "Epoch 22/50\n",
            "62/62 [==============================] - 15s 248ms/step - loss: 0.5101 - accuracy: 0.7726\n",
            "Epoch 23/50\n",
            "62/62 [==============================] - 15s 246ms/step - loss: 0.5061 - accuracy: 0.7670\n",
            "Epoch 24/50\n",
            "62/62 [==============================] - 15s 248ms/step - loss: 0.4978 - accuracy: 0.7711\n",
            "Epoch 25/50\n",
            "62/62 [==============================] - 15s 247ms/step - loss: 0.4959 - accuracy: 0.7625\n",
            "Epoch 26/50\n",
            "62/62 [==============================] - 15s 248ms/step - loss: 0.4945 - accuracy: 0.7838\n",
            "Epoch 27/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.6002 - accuracy: 0.7808\n",
            "Epoch 28/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.5312 - accuracy: 0.7859\n",
            "Epoch 29/50\n",
            "62/62 [==============================] - 16s 252ms/step - loss: 0.5158 - accuracy: 0.7686\n",
            "Epoch 30/50\n",
            "62/62 [==============================] - 16s 253ms/step - loss: 0.5050 - accuracy: 0.7762\n",
            "Epoch 31/50\n",
            "62/62 [==============================] - 16s 262ms/step - loss: 0.4988 - accuracy: 0.7625\n",
            "Epoch 32/50\n",
            "62/62 [==============================] - 16s 254ms/step - loss: 0.4948 - accuracy: 0.7787\n",
            "Epoch 33/50\n",
            "62/62 [==============================] - 18s 289ms/step - loss: 0.4852 - accuracy: 0.7716\n",
            "Epoch 34/50\n",
            "62/62 [==============================] - 16s 252ms/step - loss: 0.4791 - accuracy: 0.7772\n",
            "Epoch 35/50\n",
            "62/62 [==============================] - 16s 253ms/step - loss: 0.4762 - accuracy: 0.7645\n",
            "Epoch 36/50\n",
            "62/62 [==============================] - 16s 250ms/step - loss: 0.4718 - accuracy: 0.7843\n",
            "Epoch 37/50\n",
            "62/62 [==============================] - 16s 251ms/step - loss: 0.4631 - accuracy: 0.7762\n",
            "Epoch 38/50\n",
            "62/62 [==============================] - 15s 250ms/step - loss: 0.4606 - accuracy: 0.7823\n",
            "Epoch 39/50\n",
            "62/62 [==============================] - 16s 252ms/step - loss: 0.4555 - accuracy: 0.7854\n",
            "Epoch 40/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.4550 - accuracy: 0.7930\n",
            "Epoch 41/50\n",
            "62/62 [==============================] - 16s 253ms/step - loss: 0.4469 - accuracy: 0.7909\n",
            "Epoch 42/50\n",
            "62/62 [==============================] - 15s 250ms/step - loss: 0.4480 - accuracy: 0.7859\n",
            "Epoch 43/50\n",
            "62/62 [==============================] - 16s 250ms/step - loss: 0.4344 - accuracy: 0.7889\n",
            "Epoch 44/50\n",
            "62/62 [==============================] - 15s 250ms/step - loss: 0.4296 - accuracy: 0.7960\n",
            "Epoch 45/50\n",
            "62/62 [==============================] - 16s 251ms/step - loss: 0.4278 - accuracy: 0.7976\n",
            "Epoch 46/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.4235 - accuracy: 0.7976\n",
            "Epoch 47/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.4182 - accuracy: 0.7909\n",
            "Epoch 48/50\n",
            "62/62 [==============================] - 16s 250ms/step - loss: 0.4147 - accuracy: 0.8037\n",
            "Epoch 49/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.4072 - accuracy: 0.7976\n",
            "Epoch 50/50\n",
            "62/62 [==============================] - 15s 249ms/step - loss: 0.4051 - accuracy: 0.7970\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec4dafea50>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "print('Train...')\n",
        "model.fit(train_X, train_Y,epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJpAQrF2_eOj",
        "outputId": "8c7cd5f9-b4b5-4603-9102-6472e4ea4cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 32)                36224     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,290\n",
            "Trainable params: 36,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf_Vaei__eOk"
      },
      "source": [
        "# Test\n",
        "your model can predict correctly even for unseen words from training.  \n",
        "This is the most benefit of using pretrained word embedding.  \n",
        "Why? pretrained embedding will encode [better], [nice] to similar vector of [best]  \n",
        "even if these words were not in train.  \n",
        "therefore, the input vector to RNN is similar, so correct answers for even these unseen words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_iOjy4G_eOl",
        "outputId": "8bf23b1b-7a74-4707-c00a-fbbeb3dd05f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive    994\n",
            "neutral     681\n",
            "negative    325\n",
            "Name: label, dtype: int64\n",
            "63/63 - 5s - loss: 0.9111 - accuracy: 0.7110 - 5s/epoch - 82ms/step\n",
            "Test score: 0.9111069440841675\n",
            "Test accuracy: 0.7110000252723694\n"
          ]
        }
      ],
      "source": [
        "test_df=testData;\n",
        "print(test_df.label.value_counts())\n",
        "\n",
        "test_X, test_Y = preprocess(test_df)\n",
        "\n",
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2dzRhVYGGxpA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "dataSet_2017_Word2Vec_LSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}