{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijitgayen/SNA_As_Msc_projects/blob/main/dataSet_2017_Word2Vec_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJcGdmRg_eN6"
      },
      "source": [
        "# Sentence Classification using LSTM and Pretrained Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F61kUpXW_eN8"
      },
      "source": [
        "We will train and test sentence classification using LSTM, and Pretrained Word2Vec.  \n",
        "You can find visualization of our code below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Add libary"
      ],
      "metadata": {
        "id": "AbaDII3m_uMZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eSk_2_1n_eOV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3Wzwv4cM_eOX"
      },
      "outputs": [],
      "source": [
        "# Load Pretrained Word2Vec\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some functions"
      ],
      "metadata": {
        "id": "aD0_wW5a_oFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UHI0i3i0_eOY"
      },
      "outputs": [],
      "source": [
        "def get_max_length(df):\n",
        "    \"\"\"\n",
        "    get max token counts from train data, \n",
        "    so we use this number as fixed length input to RNN cell\n",
        "    \"\"\"\n",
        "    max_length = 0\n",
        "    for row in df['tweet']:\n",
        "        if len(row.split(\" \")) > max_length:\n",
        "            max_length = len(row.split(\" \"))\n",
        "    return max_length\n",
        "\n",
        "def get_word2vec_enc(reviews):\n",
        "    \"\"\"\n",
        "    get word2vec value for each word in sentence.\n",
        "    concatenate word in numpy array, so we can use it as RNN input\n",
        "    \"\"\"\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        tokens = review.split(\" \")\n",
        "        word2vec_embedding = embed(tokens)\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "        \n",
        "def get_padded_encoded_reviews(encoded_reviews):\n",
        "    \"\"\"\n",
        "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
        "    \"\"\"\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return padded_reviews_encoding\n",
        "\n",
        "def sentiment_encode(sentiment):\n",
        "    \"\"\"\n",
        "    return one hot encoding for Y value\n",
        "    \"\"\"\n",
        "    if sentiment == 'positive':\n",
        "        return [1,0]\n",
        "    if sentiment == 'neutral':\n",
        "        return [0.5,0.5]\n",
        "    else:\n",
        "        return [0,1]\n",
        "    \n",
        "def preprocess(df):\n",
        "    \"\"\"\n",
        "    encode text value to numeric value\n",
        "    \"\"\"\n",
        "    # encode words into word2vec\n",
        "    reviews = df['tweet'].tolist()\n",
        "    \n",
        "    encoded_reviews = get_word2vec_enc(reviews)\n",
        "    padded_encoded_reviews = get_padded_encoded_reviews(encoded_reviews)\n",
        "    # encoded sentiment\n",
        "    sentiments = df['label'].tolist()\n",
        "    encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]\n",
        "    X = np.array(padded_encoded_reviews)\n",
        "    Y = np.array(encoded_sentiment)\n",
        "    return X, Y "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8-YLayo_eOb"
      },
      "source": [
        "# Preprocess (encode text to number)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HOhHRh-D92vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetsDatas=pd.read_csv('/content/drive/MyDrive/2017_data_set/dataset/train/twitter-2016dev-A.txt', sep=\"\\t\", header=None, names=[\"id\",\"label\", \"tweet\"]); #skiping these two rows as they have some bad data\n",
        "testData=pd.read_csv('/content/drive/MyDrive/2017_data_set/dataset/train/twitter-2016devtest-A.txt',sep=\"\\t\", header=None, names=[\"id\",\"label\", \"tweet\"]); #test_data\n",
        "print(tweetsDatas.head())\n",
        "print(tweetsDatas.label.value_counts())\n",
        "print(testData.head())\n",
        "print(testData.label.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFgFx5fUByuu",
        "outputId": "0416c9b1-78cc-4aa0-b505-d2894cde5091"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   id     label  \\\n",
            "0  638060586258038784   neutral   \n",
            "1  638061181823922176  positive   \n",
            "2  638083821364244480   neutral   \n",
            "3  638091450132078593  positive   \n",
            "4  638125563790557184  positive   \n",
            "\n",
            "                                               tweet  \n",
            "0  05 Beat it - Michael Jackson - Thriller (25th ...  \n",
            "1  Jay Z joins Instagram with nostalgic tribute t...  \n",
            "2  Michael Jackson: Bad 25th Anniversary Edition ...  \n",
            "3  I liked a @YouTube video http://t.co/AaR3pjp2P...  \n",
            "4  18th anniv of Princess Diana's death. I still ...  \n",
            "positive    829\n",
            "neutral     746\n",
            "negative    391\n",
            "Name: label, dtype: int64\n",
            "                   id     label  \\\n",
            "0  637641175948763136   neutral   \n",
            "1  637651487762554881   neutral   \n",
            "2  637666734300905472  negative   \n",
            "3  637668142110654468   neutral   \n",
            "4  637708370129125377  positive   \n",
            "\n",
            "                                               tweet  \n",
            "0  @SeeMonterey LOST - Sony cell phone with holid...  \n",
            "1  @PersonaSoda well yeah, that's third parties. ...  \n",
            "2  Sony rewards app is like a lot of 19 y.o femal...  \n",
            "3  @fakethom Have android tab and don't use phone...  \n",
            "4  Finally I get my ps4 back I sent it to Sony ca...  \n",
            "positive    994\n",
            "neutral     681\n",
            "negative    325\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5a6PI6pD_eOc"
      },
      "outputs": [],
      "source": [
        "df=tweetsDatas\n",
        "# max_length is used for max sequence of input\n",
        "max_length = get_max_length(df)\n",
        "print(max_length)\n",
        "\n",
        "train_X, train_Y = preprocess(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X)"
      ],
      "metadata": {
        "id": "vALH4EdmIhdU",
        "outputId": "9b7e382d-1082-41a1-eab9-905f07ee0ce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [ 0.04849368  0.02253552 -0.03001837 ...  0.01148417 -0.00711913\n",
            "    0.07169744]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.08257177 -0.00750593  0.01259988 ... -0.01107846  0.09370738\n",
            "   -0.05649239]\n",
            "  [ 0.04446086 -0.02164922 -0.08020048 ... -0.01608738  0.08119389\n",
            "    0.02990472]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.06397844  0.06924438 -0.01906524 ...  0.06709331  0.05260783\n",
            "    0.01745548]\n",
            "  [-0.06490663 -0.04434662 -0.04786254 ... -0.00385669  0.01662072\n",
            "    0.01315502]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.01905521 -0.0505188   0.02565561 ... -0.00141199  0.05673076\n",
            "   -0.03743587]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  ...\n",
            "  [-0.08018466  0.05793865 -0.04210952 ...  0.03982512  0.07958872\n",
            "   -0.06999146]\n",
            "  [-0.00483655 -0.06458678 -0.02172526 ...  0.02054301  0.0117408\n",
            "    0.01072438]\n",
            "  [-0.00426278 -0.02541808 -0.01327157 ...  0.03809534 -0.02657138\n",
            "    0.03662907]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0REz_zM_eOd"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VhNRBzA1_eOf"
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efpd3TF__eOh"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRO932fcAVdF",
        "outputId": "57b5e690-19a4-448c-bfd1-d2aaeb4efb9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [0.5, 0.5],\n",
              "       ...,\n",
              "       [0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [1. , 0. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He_9VTLk_eOi",
        "outputId": "5641c4ef-c694-4d3a-dc11-c904eeb2dc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/50\n",
            "62/62 [==============================] - 20s 272ms/step - loss: 0.6523 - accuracy: 0.7925\n",
            "Epoch 2/50\n",
            "62/62 [==============================] - 17s 275ms/step - loss: 0.6141 - accuracy: 0.7375\n",
            "Epoch 3/50\n",
            "62/62 [==============================] - 18s 286ms/step - loss: 0.5941 - accuracy: 0.7533\n",
            "Epoch 4/50\n",
            "62/62 [==============================] - 17s 276ms/step - loss: 0.5820 - accuracy: 0.7604\n",
            "Epoch 5/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5721 - accuracy: 0.7569\n",
            "Epoch 6/50\n",
            "62/62 [==============================] - 17s 277ms/step - loss: 0.5651 - accuracy: 0.7518\n",
            "Epoch 7/50\n",
            "62/62 [==============================] - 17s 270ms/step - loss: 0.5628 - accuracy: 0.7569\n",
            "Epoch 8/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5630 - accuracy: 0.7528\n",
            "Epoch 9/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5535 - accuracy: 0.7538\n",
            "Epoch 10/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5501 - accuracy: 0.7609\n",
            "Epoch 11/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5455 - accuracy: 0.7620\n",
            "Epoch 12/50\n",
            "62/62 [==============================] - 17s 271ms/step - loss: 0.5421 - accuracy: 0.7620\n",
            "Epoch 13/50\n",
            "62/62 [==============================] - 17s 272ms/step - loss: 0.5391 - accuracy: 0.7655\n",
            "Epoch 14/50\n",
            "62/62 [==============================] - 17s 270ms/step - loss: 0.5330 - accuracy: 0.7640\n",
            "Epoch 15/50\n",
            "62/62 [==============================] - 17s 270ms/step - loss: 0.5408 - accuracy: 0.7706\n",
            "Epoch 16/50\n",
            "62/62 [==============================] - 17s 280ms/step - loss: 0.5278 - accuracy: 0.7675\n",
            "Epoch 17/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5283 - accuracy: 0.7777\n",
            "Epoch 18/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5205 - accuracy: 0.7742\n",
            "Epoch 19/50\n",
            "62/62 [==============================] - 17s 272ms/step - loss: 0.5190 - accuracy: 0.7614\n",
            "Epoch 20/50\n",
            "62/62 [==============================] - 17s 277ms/step - loss: 0.5220 - accuracy: 0.7696\n",
            "Epoch 21/50\n",
            "62/62 [==============================] - 17s 271ms/step - loss: 0.5103 - accuracy: 0.7813\n",
            "Epoch 22/50\n",
            "62/62 [==============================] - 17s 275ms/step - loss: 0.5088 - accuracy: 0.7747\n",
            "Epoch 23/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.5040 - accuracy: 0.7762\n",
            "Epoch 24/50\n",
            "62/62 [==============================] - 18s 285ms/step - loss: 0.4985 - accuracy: 0.7665\n",
            "Epoch 25/50\n",
            "62/62 [==============================] - 17s 280ms/step - loss: 0.4949 - accuracy: 0.7777\n",
            "Epoch 26/50\n",
            "62/62 [==============================] - 17s 273ms/step - loss: 0.4882 - accuracy: 0.7737\n",
            "Epoch 27/50\n",
            "62/62 [==============================] - 17s 275ms/step - loss: 0.4882 - accuracy: 0.7731\n",
            "Epoch 28/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.4771 - accuracy: 0.7731\n",
            "Epoch 29/50\n",
            "62/62 [==============================] - 17s 275ms/step - loss: 0.4739 - accuracy: 0.7691\n",
            "Epoch 30/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.4737 - accuracy: 0.7747\n",
            "Epoch 31/50\n",
            "62/62 [==============================] - 17s 280ms/step - loss: 0.4801 - accuracy: 0.7772\n",
            "Epoch 32/50\n",
            "62/62 [==============================] - 17s 272ms/step - loss: 0.4748 - accuracy: 0.7635\n",
            "Epoch 33/50\n",
            "62/62 [==============================] - 17s 272ms/step - loss: 0.4574 - accuracy: 0.7757\n",
            "Epoch 34/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.4501 - accuracy: 0.7772\n",
            "Epoch 35/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.4452 - accuracy: 0.7721\n",
            "Epoch 36/50\n",
            "62/62 [==============================] - 17s 275ms/step - loss: 0.4448 - accuracy: 0.7686\n",
            "Epoch 37/50\n",
            "62/62 [==============================] - 17s 281ms/step - loss: 0.4426 - accuracy: 0.7686\n",
            "Epoch 38/50\n",
            "62/62 [==============================] - 17s 282ms/step - loss: 0.4386 - accuracy: 0.7798\n",
            "Epoch 39/50\n",
            "62/62 [==============================] - 17s 277ms/step - loss: 0.4301 - accuracy: 0.7686\n",
            "Epoch 40/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.4317 - accuracy: 0.7808\n",
            "Epoch 41/50\n",
            "62/62 [==============================] - 17s 278ms/step - loss: 0.4227 - accuracy: 0.7767\n",
            "Epoch 42/50\n",
            "62/62 [==============================] - 17s 277ms/step - loss: 0.4276 - accuracy: 0.7823\n",
            "Epoch 43/50\n",
            "62/62 [==============================] - 17s 282ms/step - loss: 0.4195 - accuracy: 0.7848\n",
            "Epoch 44/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.4121 - accuracy: 0.7879\n",
            "Epoch 45/50\n",
            "62/62 [==============================] - 17s 276ms/step - loss: 0.4044 - accuracy: 0.7777\n",
            "Epoch 46/50\n",
            "62/62 [==============================] - 17s 277ms/step - loss: 0.4094 - accuracy: 0.7823\n",
            "Epoch 47/50\n",
            "62/62 [==============================] - 17s 272ms/step - loss: 0.3977 - accuracy: 0.7823\n",
            "Epoch 48/50\n",
            "62/62 [==============================] - 17s 274ms/step - loss: 0.3996 - accuracy: 0.7894\n",
            "Epoch 49/50\n",
            "62/62 [==============================] - 17s 273ms/step - loss: 0.3886 - accuracy: 0.7899\n",
            "Epoch 50/50\n",
            "62/62 [==============================] - 17s 278ms/step - loss: 0.3910 - accuracy: 0.7782\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcfdcc4fd10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "print('Train...')\n",
        "model.fit(train_X, train_Y,epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJpAQrF2_eOj",
        "outputId": "4a703be0-39f4-4916-b076-d102a6534447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 32)                36224     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,290\n",
            "Trainable params: 36,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf_Vaei__eOk"
      },
      "source": [
        "# Test\n",
        "your model can predict correctly even for unseen words from training.  \n",
        "This is the most benefit of using pretrained word embedding.  \n",
        "Why? pretrained embedding will encode [better], [nice] to similar vector of [best]  \n",
        "even if these words were not in train.  \n",
        "therefore, the input vector to RNN is similar, so correct answers for even these unseen words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_iOjy4G_eOl",
        "outputId": "bc911573-ea08-4f5a-ccc9-9cf716dfa0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive    994\n",
            "neutral     681\n",
            "negative    325\n",
            "Name: label, dtype: int64\n",
            "63/63 - 6s - loss: 0.9267 - accuracy: 0.6850 - 6s/epoch - 89ms/step\n",
            "Test score: 0.9267452955245972\n",
            "Test accuracy: 0.6850000023841858\n"
          ]
        }
      ],
      "source": [
        "test_df=testData;\n",
        "print(test_df.label.value_counts())\n",
        "\n",
        "test_X, test_Y = preprocess(test_df)\n",
        "\n",
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2dzRhVYGGxpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df=testData\n",
        "print(test_df.lebal.value_counts())\n",
        "\n",
        "test_X, test_Y = preprocess(test_df)\n",
        "\n",
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "lwxxgYLUG3OU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "dataSet_2017_Word2Vec_LSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}